{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb5b92dd",
   "metadata": {},
   "source": [
    "# Reproduce the results presented in \"CRISPR-HAWK: Haplotype- and Variant-Aware Guide Design Toolkit for CRISPR-Cas\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48317ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import subprocess\n",
    "import random\n",
    "import pysam\n",
    "import os\n",
    "\n",
    "from time import time\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.font_manager import FontProperties\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b930a4",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "CRISPR-HAWK is a comprehensive and scalable framework for designing guide RNAs \n",
    "(gRNAs) and evaluating the impact of genetic variation on CRISPR-Cas on-target \n",
    "activity. Developed as an offline, user-friendly command-line tool, CRISPR-HAWK \n",
    "integrates large-scale human variation datasets, including the 1000 Genomes Project, \n",
    "the Human Genome Diversity Project (HGDP), and gnomAD, with orthogonal \n",
    "genomic annotations to systematically prioritize gRNAs targeting regions of interest.\n",
    "\n",
    "The framework is Cas-agnostic and supports a broad range of nucleases, such as \n",
    "Cas9, SaCas9, and Cpf1 (Cas12a), while also allowing full customization of PAM \n",
    "sequences and guide lengths. This flexibility ensures compatibility with emerging \n",
    "CRISPR technologies and enables users to tailor gRNA design to specific experimental \n",
    "needs.\n",
    "\n",
    "CRISPR-HAWK incorporates both single-nucleotide variants (SNVs) and small \n",
    "insertions and deletions (indels), and it natively handles individual- and \n",
    "population-specific haplotypes. This makes it particularly suitable for personalized \n",
    "genome editing as well as population-scale analyses. The workflow, from variant-aware \n",
    "preprocessing to gRNA discovery, is fully automated, generating ranked candidate \n",
    "gRNAs, annotated target sequences, and publication-ready visualizations.\n",
    "\n",
    "Thanks to its modular architecture, CRISPR-HAWK can be seamlessly integrated with \n",
    "downstream tools such as CRISPRme or CRISPRitz for comprehensive off-target prediction \n",
    "and follow-up analysis of prioritized guides.\n",
    "\n",
    "This notebook reproduce the results presented in |||**add paper-link**|||."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26072b6",
   "metadata": {},
   "source": [
    "## Download data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55f51b00",
   "metadata": {},
   "source": [
    "### Downloading Genetic Variation Datasets (1000 Genomes, HGDP, gnomAD)\n",
    "\n",
    "CRISPR-HAWK supports large-scale genetic diversity analyses by integrating variation \n",
    "from several major population genomics resources. This section provides instructions \n",
    "for downloading the VCF files required to reproduce the results presented in the paper.\n",
    "\n",
    "**Overview of Supported Datasets**\n",
    "\n",
    "- 1000 Genomes Project (Phase 3)\n",
    "  <br>2,504 individuals from 26 global populations; whole-genome sequencing at ~30×.\n",
    "\n",
    "- Human Genome Diversity Project (HGDP)\n",
    "  <br>929 individuals from globally diverse populations; high-coverage WGS.\n",
    "\n",
    "- gnomAD (v4.1)\n",
    "  <br>Population-scale aggregated variation from ~76,000 genomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b4b94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define datasets urls\n",
    "url_hgdp = (\n",
    "    \"https://ngs.sanger.ac.uk/production/hgdp/hgdp_wgs.20190516/\" \n",
    "    \"hgdp_wgs.20190516.full.chr{}.vcf.gz\"\n",
    ")\n",
    "url_1kgp = (\n",
    "    \"ftp://ftp.1000genomes.ebi.ac.uk/vol1/ftp/data_collections/\"\n",
    "    \"1000_genomes_project/release/20190312_biallelic_SNV_and_INDEL/\" \n",
    "    \"ALL.chr{}.shapeit2_integrated_snvindels_v2a_27022019.GRCh38.phased.vcf.gz\"\n",
    ")\n",
    "url_gnomad = (\n",
    "    \"https://storage.googleapis.com/gcp-public-data--gnomad/release/4.1/vcf/\" \n",
    "    \"genomes/gnomad.genomes.v4.1.sites.chr{}.vcf.bgz\"\n",
    ")\n",
    "variants = {\"HGDP\": url_hgdp, \"1000G\": url_1kgp, \"GNOMAD\": url_gnomad}\n",
    "\n",
    "# define chromosomes\n",
    "chroms = [2, 3, 7, 11]\n",
    "\n",
    "# download files\n",
    "for dataset, url in variants.items():\n",
    "    vcfdir = os.path.join(\"vcf\", dataset)\n",
    "    if dataset == \"GNOMAD\":\n",
    "        vcfdir = os.path.join(vcfdir, \"raw\")\n",
    "    os.makedirs(vcfdir, exist_ok=True)  # create dataset folder \n",
    "    for c in chroms:        \n",
    "        # download VCF and index\n",
    "        ! wget -P {vcfdir} {url.format(c)}\n",
    "        ! wget -P {vcfdir} {url.format(c)}.tbi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6f2b59c",
   "metadata": {},
   "source": [
    "Unlike the 1000 Genomes and HGDP callsets, the gnomAD VCFs contain allele frequency \n",
    "information only and do not provide individual-level genotype data. Since \n",
    "CRISPR-HAWK requires genotypes to reconstruct haplotypes and perform variant-aware \n",
    "gRNA discovery, gnomAD VCFs must first be converted into a compatible, pseudo-genotyped \n",
    "format. To enable this, we use the CRISPR-HAWK VCF converter, which generates a \n",
    "CRISPR-HAWK–ready VCF while preserving population-level allele distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd930da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gnomad folder exists\n",
    "gnomad_dir = \"vcf/GNOMAD\"\n",
    "gnomad_raw_dir = os.path.join(gnomad_dir, \"raw\")\n",
    "assert os.path.isdir(gnomad_raw_dir)\n",
    "\n",
    "# create converted VCFs folder\n",
    "gnomad_gt_dir = os.path.join(gnomad_dir, \"genotype\")\n",
    "os.makedirs(gnomad_gt_dir, exist_ok=True)\n",
    "\n",
    "# convert gnoamd VCFs using crisprhawk (it may take some time)\n",
    "! crisprhawk convert-gnomad-vcf -d {gnomad_raw_dir} -o {gnomad_gt_dir}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a6a36a1",
   "metadata": {},
   "source": [
    "### Downloading the hg38 Reference Genome\n",
    "\n",
    "CRISPR-HAWK requires a reference genome to extract genomic contexts, evaluate \n",
    "variant-aware target sequences, and correctly map gRNA target sites. In this \n",
    "section, we download the primary assembly FASTA filesfrom the Genome Reference \n",
    "Consortium–maintained repositories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d9e1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define chromosomes\n",
    "chroms = [2, 3, 7, 11]\n",
    "\n",
    "# create genome folder\n",
    "genome_dir = \"genome\"\n",
    "os.makedirs(genome_dir, exist_ok=True)\n",
    "\n",
    "# define genome url\n",
    "url_ucsc = (\n",
    "    \"https://hgdownload.soe.ucsc.edu/goldenpath/hg38/chromosomes/chr{}.fa.gz\"\n",
    ")\n",
    "\n",
    "# Download and unzip FASTA file for each chromosome\n",
    "for c in chroms:\n",
    "    print(f\"Downloading FASTA for chromosome {c}\")\n",
    "    ! wget -nc -P {genome_dir} {url_ucsc.format(c)}\n",
    "    ! gunzip -f {genome_dir}/chr{c}.fa.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f39fe4",
   "metadata": {},
   "source": [
    "### Creating BED Files for the Analyzed Regions\n",
    "\n",
    "CRISPR-HAWK requires BED files to define the genomic intervals where gRNA discovery \n",
    "and variant-aware analysis will be performed. Each BED file specifies one or more \n",
    "regions of interest in the standard 3-column BED format:\n",
    "```\n",
    "chrom   start   end\n",
    "```\n",
    "\n",
    "Coordinates must reference the hg38 genome assembly (or whichever reference you \n",
    "downloaded in the previous step)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b476416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create region folder\n",
    "regions_dir = \"regions\"\n",
    "os.makedirs(regions_dir, exist_ok=True)\n",
    "\n",
    "# define regions\n",
    "regions = {\n",
    "    \"BCL11A\": [\"chr2\", 60495215, 60496479],\n",
    "    \"EMX1\":   [\"chr2\", 72932853, 72934853],\n",
    "    \"CCR5_1\":   [\"chr3\", 46372138, 46374138],\n",
    "    \"CCR5_2\":   [\"chr3\", 46372162, 46374162],\n",
    "    \"TRBC1\":  [\"chr7\", 142791004, 142793004],\n",
    "    \"TRBC2\":  [\"chr7\", 142800351, 142802350],\n",
    "    \"HBB_1\":    [\"chr11\", 5225803, 5227803],\n",
    "    \"HBB_2\":    [\"chr11\", 5225967, 5227967],\n",
    "    \"HBG2_CAS9\":   [\"chr11\", 5252879, 5256879],\n",
    "    \"HBG1_CAS9\":   [\"chr11\", 5248955, 5250955],\n",
    "    \"HBG2_CPF1\":   [\"chr11\", 5253874, 5255874],\n",
    "    \"HBG1_CPF1\":   [\"chr11\", 5248950, 5250950],\n",
    "    \"FANCF\":  [\"chr11\", 22624785, 22626785],\n",
    "}\n",
    "\n",
    "# create bed files \n",
    "for gene, (chrom, start, end) in regions.items():\n",
    "    bed_fname = os.path.join(regions_dir, f\"{gene}.bed\")\n",
    "    with open(bed_fname, mode=\"w\") as f:\n",
    "        f.write(f\"{chrom}\\t{start}\\t{end}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ef53cb",
   "metadata": {},
   "source": [
    "## Variant-Aware gRNA Retrieval on Defined Regions\n",
    "\n",
    "With the reference genome, variant datasets, and BED files prepared, CRISPR-HAWK\n",
    "can now retrieve all candidate gRNAs within the specified regions while accounting\n",
    "for genetic variation across populations and individuals. This step is central to \n",
    "CRISPR-HAWK’s design philosophy: guide discovery must be variant-aware, ensuring \n",
    "that both reference and haplotype-specific target sequences are evaluated.\n",
    "\n",
    "**What This Step Does**\n",
    "\n",
    "For each genomic interval listed in the BED file(s), CRISPR-HAWK:\n",
    "\n",
    "- Extracts the reference sequence from hg38.\n",
    "\n",
    "- Applies all relevant variants (SNVs and indels) from the loaded dataset,\n",
    "including 1000G, HGDP, or converted gnomAD, to reconstruct individual, and population-specific haplotypes.\n",
    "\n",
    "- Scans both the reference and haplotype sequences to identify all gRNAs that match the user-defined:\n",
    "\n",
    "    - PAM sequence (e.g., NGG, NAA, TTTV, …)\n",
    "\n",
    "    - guide length\n",
    "\n",
    "    - nuclease type (Cas9, SaCas9, Cpf1/Cas12a, etc.)\n",
    "\n",
    "- Reports each gRNA along with:\n",
    "\n",
    "    - its exact reference and alternative alleles\n",
    "\n",
    "    - per-haplotype presence/absence\n",
    "\n",
    "    - the samples in which the target site is altered\n",
    "\n",
    "    - summary metrics for prioritization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2587f587",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define target regions\n",
    "targets = [\n",
    "    \"BCL11A\",\n",
    "    \"EMX1\",\n",
    "    \"CCR5_1\",\n",
    "    \"CCR5_2\",\n",
    "    \"TRBC1\",\n",
    "    \"TRBC2\",\n",
    "    \"FANCF\",\n",
    "    \"HBB_1\",\n",
    "    \"HBB_2\",\n",
    "    \"HBG1_CAS9\",\n",
    "    \"HBG2_CAS9\",\n",
    "    \"HBG1_CPF1\",\n",
    "    \"HBG1_CPF1\",\n",
    "]\n",
    "\n",
    "# define genetic variants datasets\n",
    "datasets = [\"1000G\", \"HGDP\", \"GNOMAD\"]\n",
    "\n",
    "# define pams\n",
    "pams = [\"NGG\", \"TTTV\"] # Cas9, Cas12\n",
    "guide_lens = [20, 23]\n",
    "thread = 16\n",
    "\n",
    "# define folders \n",
    "genome_dir = \"genome\"\n",
    "variants_dir = \"vcf\"\n",
    "regions_dir = \"regions\"\n",
    "results_dir = \"results\"\n",
    "\n",
    "# create results folder\n",
    "os.makedirs(results_dir, exist_ok=True)\n",
    "\n",
    "# run guide design with crisprhawk\n",
    "for dataset in datasets:\n",
    "    vcfdir = os.path.join(variants_dir, dataset)\n",
    "    if dataset == \"GNOMAD\":\n",
    "        vcfdir = os.path.join(vcfdir, \"genotype\")\n",
    "    results_dataset = os.path.join(results_dir, dataset)\n",
    "    os.makedirs(results_dataset, exist_ok=True)  # e.g. results/1000G \n",
    "    for target in targets:\n",
    "        target_region = os.path.join(regions_dir, f\"{target}.bed\")\n",
    "        pam = pams[1] if target.endswith(\"_CPF1\") else pams[0]\n",
    "        results_target = os.path.join(results_dataset, target)  # e.g. results/1000G/BCL11A\n",
    "        guidelen = 20 if pam == \"NGG\" else 23\n",
    "        crisprhawk_cmd = (\n",
    "            \"crisprhawk search \" \n",
    "            f\"-f {genome_dir} \" \n",
    "            f\"-r {target_region} \"\n",
    "            f\"-v {vcfdir} \"\n",
    "            f\"-p {pam} \"\n",
    "            f\"-g {guidelen} \"\n",
    "            f\"--haplotype-table \"\n",
    "            \"--threads 16 \"\n",
    "            f\"-o {results_target}\"\n",
    "        )\n",
    "        if pam == \"TTTV\":\n",
    "            crisprhawk_cmd += \" --right\"  # pam upstream spacer\n",
    "        print(f\"Running search on {dataset} for target {target}\")\n",
    "        subprocess.call(crisprhawk_cmd, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0583cf",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization\n",
    "\n",
    "In this section, we analyze and visualize the impact of genetic variation on gRNA \n",
    "design and activity across a curated set of clinically and experimentally relevant \n",
    "CRISPR targets. The analysis focuses on how population-level and individual-specific \n",
    "variants affect both gRNA sequence composition and expected on-target efficiency, \n",
    "highlighting differences between reference-designed guides and their alternative, \n",
    "haplotype-derived counterparts.\n",
    "\n",
    "For each target region, we first quantify how genetic variation alters the landscape \n",
    "of candidate gRNAs. Specifically, we classify retrieved guides into four categories \n",
    "and summarize them using pie charts:\n",
    "- gRNAs matching the reference sequence\n",
    "- gRNAs with variants in the spacer region\n",
    "- gRNAs with variants affecting only the PAM\n",
    "- gRNAs with variants in both the spacer and the PAM\n",
    "\n",
    "This provides an immediate overview of how frequently genetic variants modify \n",
    "targetable sequences across different loci. \n",
    "\n",
    "Next, we assess the functional consequences of these sequence differences. Using \n",
    "dot plots, we compare:\n",
    "- The predicted on-target efficiency of reference gRNAs versus their alternative \n",
    "  versions found on variant-defined haplotypes\n",
    "- The residual on-target activity of reference gRNAs when applied to alternative \n",
    "  haplotypes carrying sequence mismatches\n",
    "\n",
    "These analyses capture both gain and loss of activity induced by genetic variation \n",
    "and enable a fine-grained comparison between reference and variant-aware gRNA designs.\n",
    "\n",
    "All analyses are performed independently for the following target regions:\n",
    "- BCL11A +58 Erythroid enhancer\n",
    "- EMX1\n",
    "- CCR5 (two independent target sites)\n",
    "- TRBC1\n",
    "- TRBC2\n",
    "- FANCF\n",
    "- HBB (two independent target sites)\n",
    "- HBG1 and HBG2 (Cas9)\n",
    "- HBG1 (Cpf1/Cas12a)\n",
    "\n",
    "For Cpf1-based targets, residual on-target activity is not evaluated, as the \n",
    "analysis is specific to Cas9-mediated spacer–PAM interactions.\n",
    "\n",
    "The analyses integrate variation from 1000 Genomes, HGDP, and gnomAD datasets. \n",
    "In particular, for the sg1617 guide targeting the BCL11A erythroid enhancer, we \n",
    "perform an in-depth follow-up analysis: for each alternative gRNA sequence generated \n",
    "by gnomAD variants, we run CRISPRme (using 1000G + HGDP genetic variants) to \n",
    "evaluate guide specificity genome-wide. This allows us to compare how genetic \n",
    "variation simultaneously affects on-target activity and off-target risk, \n",
    "providing a comprehensive assessment of guide performance in a population-aware \n",
    "context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323891a7",
   "metadata": {},
   "source": [
    "### Assigning Sample Support to Candidate gRNAs\n",
    "\n",
    "As a first step in the results analysis, we quantify how widely each candidate \n",
    "gRNA is supported across individuals in each variant datasets. For every gRNA \n",
    "retrieved in the previous step, we compute the number of samples carrying the \n",
    "exact gRNA sequence in their reconstructed haplotypes.\n",
    "\n",
    "These sample support counts form the basis for all downstream analyses in this \n",
    "section, including the evaluation of efficiency differences between reference \n",
    "and alternative guides, and reference on-target residual activity on alternative\n",
    "haplotypes. \n",
    "\n",
    "For datasets providing individual-level genotypes (e.g., 1000 Genomes and HGDP), \n",
    "sample support is directly derived from haplotype reconstruction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e621c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLESNUM = {\"1000G\": 2504, \"HGDP\": 929, \"GNOMAD\": 76215}\n",
    "\n",
    "def _compute_id(chrom, start, stop, strand):\n",
    "    return f\"{chrom}_{start}_{stop}_{strand}\"\n",
    "\n",
    "def compute_samples_num(df, samplesnum):\n",
    "    assert \"samples\" in df.columns.tolist()\n",
    "    # count samples carrying alternative grnas\n",
    "    df[\"n_samples\"] = df[\"samples\"].apply(\n",
    "        lambda x: len(str(x).split(\",\")) if pd.notna(x) and x!= \"REF\" else None\n",
    "    )  \n",
    "    # compute guide id for each grna\n",
    "    df[\"guide_id\"] = df.apply(\n",
    "        lambda x: _compute_id(x[\"chr\"], x[\"start\"], x[\"stop\"], x[\"strand\"]), axis=1\n",
    "    )\n",
    "    # sum samples carrying alternative guides\n",
    "    sum_alternative = df[df[\"samples\"] != \"REF\"].groupby(\"guide_id\")[\"n_samples\"].sum()\n",
    "    # retrieve number of samples for reference grnas\n",
    "    refmask = df[\"samples\"] == \"REF\"\n",
    "    df.loc[refmask, \"n_samples\"] = samplesnum - df.loc[refmask, \"guide_id\"].map(sum_alternative).fillna(0)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ef0f2e",
   "metadata": {},
   "source": [
    "For gnomAD-based analyses, individual genotypes are not directly available. In \n",
    "this case, CRISPR-HAWK reports the number of populations carrying each gRNA rather\n",
    "than explicit sample counts. To enable downstream analyses requiring sample-level \n",
    "support, we post-process CRISPR-HAWK reports by annotating them with gnomAD \n",
    "carrier counts using pysam. This procedure estimates the number of carriers for\n",
    "each variant allele underlying a gRNA and propagates this information to the \n",
    "guide level.\n",
    "\n",
    "These sample support estimates are then used consistently with the other datasets \n",
    "to classify gRNAs and to perform comparative analyses across reference and \n",
    "variant-derived guides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f87bc4fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gnomad-only post-processing (NB: may require hours to run)\n",
    "def _retrieve_position(variant_id):\n",
    "    return int(variant_id.split(\"-\")[1])\n",
    "\n",
    "def extract_variant_positions(df):\n",
    "    # retrieve position of each variant\n",
    "    variant_ids = df[\"variant_id\"].dropna().tolist()\n",
    "    return {_retrieve_position(v) for vs in variant_ids for v in vs.split(\",\")}\n",
    "\n",
    "def get_relevant_variants(vcf_fname, chrom, positions):\n",
    "    print(f\"Scanning VCF for {len(positions)} positions on {chrom}\")\n",
    "    vcf = pysam.VariantFile(vcf_fname)  # load vcf\n",
    "    variants, matches = [], set()\n",
    "\n",
    "    # requires bgzipped vcf with tbi index\n",
    "    for r in tqdm(vcf.fetch(chrom), desc=\"Scanning VCF\", unit=\"variants\"):\n",
    "        if r.pos not in positions:\n",
    "            continue\n",
    "    \n",
    "        ac = r.info.get(\"AC\")  # read allele count\n",
    "        if ac is None:\n",
    "            continue\n",
    "\n",
    "        nhomalt = r.info.get(\"nhomalt\", 0)  # count number of homozygous for alternative\n",
    "\n",
    "        # iterate on each alternative allele\n",
    "        for i, alt in enumerate(r.alts):\n",
    "            # scalar vs per-allele fields for AC and nhomalt\n",
    "            if isinstance(ac, (list, tuple)):\n",
    "                ac_ = ac[i] if i < len(ac) else None\n",
    "            else:\n",
    "                ac_ = ac\n",
    "            if ac_ is None:\n",
    "                continue\n",
    "\n",
    "            if isinstance(nhomalt, (list, tuple)):\n",
    "                nhomalt_ = nhomalt[i] if i < len(nhomalt) else 0\n",
    "            else:\n",
    "                nhomalt_ = nhomalt\n",
    "\n",
    "            # compute number of heterozygous samples\n",
    "            n_het = ac_ - 2 * nhomalt_\n",
    "            n_total = nhomalt_ + n_het\n",
    "\n",
    "            # add variants and matched position\n",
    "            variant_id = f\"{r.chrom}-{r.pos}-{r.ref}/{alt}\"\n",
    "            variants.append(\n",
    "                {\n",
    "                    \"variant_id\": variant_id,\n",
    "                    \"chrom\": r.chrom,\n",
    "                    \"pos\": r.pos,\n",
    "                    \"ref\": r.ref,\n",
    "                    \"alt\": alt,\n",
    "                    \"AC\": ac_,\n",
    "                    \"n_hom\": nhomalt_,\n",
    "                    \"n_het\": n_het,\n",
    "                    \"n_samples\": n_total\n",
    "                }\n",
    "            )\n",
    "            matches.add(r.pos)\n",
    "    print(f\"Matched {len(matches)} of {len(positions)} positions\")\n",
    "    return pd.DataFrame(variants)\n",
    "\n",
    "def build_samples_dict(df_variants):\n",
    "    # map variant IDs to total sample carrier counts\n",
    "    df_sums = df_variants. \\\n",
    "        groupby(\"variant_id\", sort=False)[\"n_samples\"]. \\\n",
    "        sum(). \\\n",
    "        reset_index()\n",
    "    return df_sums.set_index(\"variant_id\")[\"n_samples\"].to_dict()\n",
    "\n",
    "def get_samples_total(variant_ids, samples_dict):\n",
    "    if pd.isna(variant_ids):\n",
    "        return \"NA\"\n",
    "    total = sum(\n",
    "        int(samples_dict[v]) \n",
    "        for v in variant_ids.split(\",\") \n",
    "        if v in samples_dict\n",
    "    )\n",
    "    return str(total) if total else \"NA\"\n",
    "\n",
    "def annotate_gnomad_report(vcf_path, chrom, report_path):\n",
    "    print(\"Loading CRISPR-HAWK report...\")\n",
    "    report_df = pd.read_csv(report_path, sep=\"\\t\")\n",
    "    print(f\"Loaded {len(report_df)} rows.\")\n",
    "\n",
    "    print(\"Extracting variant positions...\")\n",
    "    positions = extract_variant_positions(report_df)\n",
    "    if not positions:\n",
    "        print(\"No valid variant positions found. Exiting.\")\n",
    "        return report_df\n",
    "\n",
    "    start = time()\n",
    "    df_variants = get_relevant_variants(vcf_path, chrom, positions)\n",
    "\n",
    "    # Write carriers file as COMMA-separated \n",
    "    variant_out = f\"variants_carriers_{chrom}_pysam.tsv\"\n",
    "    df_variants.to_csv(variant_out, index=False)\n",
    "    print(f\"Saved variant carrier data to {variant_out} in {time() - start:.2f} seconds\")\n",
    "\n",
    "    # Build samples dict from grouped sums\n",
    "    samples_dict = build_samples_dict(df_variants)\n",
    "\n",
    "    # Annotate the report\n",
    "    tqdm.pandas(desc=\"Annotating report\")\n",
    "    report_df[\"n_samples\"] = report_df[\"variant_id\"]. \\\n",
    "        progress_apply(lambda x: get_samples_total(x, samples_dict))\n",
    "    output_path = f\"{os.path.splitext(report_path)[0]}_samples_pysam.tsv\"\n",
    "    report_df.to_csv(output_path, sep=\"\\t\", index=False)\n",
    "    print(f\"\\nDone. Annotated report saved to: {output_path}\")\n",
    "\n",
    "    return report_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d81206",
   "metadata": {},
   "source": [
    "Now that we defined the functions to quantify how widely each candidate \n",
    "gRNA is supported across individuals in each variant datasets, we proceed by \n",
    "annotating BCL11A guide candidates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6429479",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_fname = \"crisprhawk_guides__chr2_60495215_60496479_NGG_20.tsv\"\n",
    "report_fname_samples = \"crisprhawk_guides__chr2_60495215_60496479_NGG_20_samples.tsv\"\n",
    "\n",
    "# 1000G \n",
    "bcl11a_1000G = pd.read_csv(f\"results/1000G/BCL11A/{report_fname}\", sep=\"\\t\")\n",
    "bcl11a_1000G = compute_samples_num(bcl11a_1000G, SAMPLESNUM[\"1000G\"])\n",
    "bcl11a_1000G.to_csv(f\"results/1000G/BCL11A/{report_fname_samples}\", sep=\"\\t\", index=False)\n",
    "\n",
    "\n",
    "# HGDP\n",
    "bcl11a_HGDP = pd.read_csv(f\"resuts/HGDP/BCL11A/{report_fname}\", sep=\"\\t\")\n",
    "bcl11a_HGDP = compute_samples_num(bcl11a_HGDP, SAMPLESNUM[\"HGDP\"])\n",
    "bcl11a_HGDP.to_csv(f\"results/HGDP/BCL11A/{report_fname_samples}.tsv\", sep=\"\\t\", index=False)\n",
    "\n",
    "# gnomAD  -- this step takes several hours to complete\n",
    "vcf_GNOMAD = \"vcf/GNOMAD/raw/gnomad.genomes.v4.1.sites.chr2.vcf.bgz\"\n",
    "report_GNOMAD = f\"results/GNOMAD/BCL11A/{report_fname}.tsv\"\n",
    "bcl11a_GNOMAD = annotate_gnomad_report(vcf_GNOMAD, \"chr2\", report_GNOMAD)\n",
    "\n",
    "bcl11a_GNOMAD = compute_samples_num(bcl11a_GNOMAD, SAMPLESNUM[\"GNOMAD\"])\n",
    "bcl11a_GNOMAD.to_csv(f\"results/GNOMAD/BCL11A/{report_fname_samples}.tsv\", sep=\"\\t\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4ce840",
   "metadata": {},
   "source": [
    "## Types of Retrieved Guide Candidates\n",
    "\n",
    "Here we classify all retrieved guide candidates according to their relationship \n",
    "with genetic variation. Each guide is assigned to one of the following categories:\n",
    "\n",
    "- Reference Guides: guides fully supported by the reference genome and unaffected \n",
    "    by observed variants.\n",
    "\n",
    "- PAM and Spacer Alternative Guides: guides that arise due to the presence of \n",
    "    genetic variants creating new protospacer or PAM sequences.\n",
    "\n",
    "- PAM Alternative Guides: guides that arise due to the presence of \n",
    "    genetic variants creating new PAM sequences.\n",
    "\n",
    "- Spacer Alternative Guides: guides that arise due to the presence of \n",
    "    genetic variants creating new protospacer sequences.\n",
    "\n",
    "The pie charts below summarize the relative proportion of each guide type among \n",
    "all retrieved candidates. This visualization provides an intuitive overview of \n",
    "how much of the guide space would be missed or mischaracterized by a reference-only \n",
    "approach and highlights the contribution of variant- and haplotype-aware guide \n",
    "discovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a5ad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "LABELS = [\n",
    "    \"Reference Guides\",\n",
    "    \"Spacer Alternative Guides\",\n",
    "    \"PAM Alternative Guides\",\n",
    "    \"PAM and Spacer Alternative Guides\"\n",
    "]\n",
    "\n",
    "def compute_guide_id_piechart(chrom, start, stop, strand, sgRNA_sequence, pam):\n",
    "    return f\"{chrom}_{start}_{stop}_{strand}_{sgRNA_sequence}_{pam}\"\n",
    "\n",
    "\n",
    "def has_lowercase(s):\n",
    "    return any(c.islower() for c in str(s))\n",
    "\n",
    "\n",
    "def retrieve_pie_chart_data(df):\n",
    "    df[\"guide_id\"] = df.apply(\n",
    "        lambda x: compute_guide_id_piechart(\n",
    "            x[\"chr\"], x[\"start\"], x[\"stop\"], x[\"strand\"], x[\"sgRNA_sequence\"], x[\"pam\"]\n",
    "        ), \n",
    "        axis=1\n",
    "    )\n",
    "    df[\"category\"] = \"Unclassified\"  # default\n",
    "\n",
    "    # remove duplicate rows (same site and spacer+pam, different haplotype and scores)\n",
    "    df = df.drop_duplicates(subset=\"guide_id\")\n",
    "\n",
    "    # category 1: Reference Guides\n",
    "    df.loc[df[\"origin\"] == \"ref\", \"category\"] = \"Reference Guides\"\n",
    "\n",
    "    # category 2: PAM and Spacer Alternative Guides\n",
    "    mask_non_ref = df[\"origin\"] != \"ref\"\n",
    "    condition2 = mask_non_ref & df[\"sgRNA_sequence\"].apply(has_lowercase) & df[\"pam\"].apply(has_lowercase)\n",
    "    df.loc[condition2, \"category\"] = \"PAM and Spacer Alternative Guides\"\n",
    "\n",
    "    # category 3: PAM Alternative Guides\n",
    "    condition3 = mask_non_ref & df[\"pam\"].apply(has_lowercase) & ~df[\"sgRNA_sequence\"].apply(has_lowercase)\n",
    "    df.loc[condition3, \"category\"] = \"PAM Alternative Guides\"\n",
    "\n",
    "    # Category 4: Spacer Alternative Guides\n",
    "    condition4 = mask_non_ref & df[\"sgRNA_sequence\"].apply(has_lowercase) & ~df[\"pam\"].apply(has_lowercase)\n",
    "    df.loc[condition4, \"category\"] = \"Spacer Alternative Guides\"\n",
    "\n",
    "    unclassified_count = (df[\"category\"] == \"Unclassified\").sum()\n",
    "    assert unclassified_count == 0, f\"{unclassified_count} guides remain unclassified!\"\n",
    "\n",
    "    return [df[\"category\"].value_counts().get(cat, 0) for cat in LABELS]\n",
    "\n",
    "\n",
    "def piechart(ax, data, title):\n",
    "    # colors for each category\n",
    "    colors = [\"#aaa1cdff\", \"#92c1deff\", \"#ef86bdff\", \"#b8d8c8\"]\n",
    "    explode = (0, 0, 0.05, 0.1)\n",
    "\n",
    "    wedges, texts, autotexts = ax.pie(\n",
    "        data,\n",
    "        explode=explode,\n",
    "        colors=colors,\n",
    "        autopct='%1.2f%%',\n",
    "        shadow=False,\n",
    "        startangle=140,\n",
    "        textprops={\"fontsize\": 16},\n",
    "        pctdistance=1.1\n",
    "    )\n",
    "\n",
    "    ax.set_title(title, fontsize=20)\n",
    "    ax.axis(\"equal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1551c00",
   "metadata": {},
   "source": [
    "Once defined the functions to plot our guide piechart, we display the gRNAs \n",
    "categories proportion in the three datasets on BCL11A enhancer region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af25a54f",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_fname = \"crisprhawk_guides__chr2_60495215_60496479_NGG_20.tsv\"\n",
    "report_fname_samples = \"crisprhawk_guides__chr2_60495215_60496479_NGG_20_samples.tsv\"\n",
    "\n",
    "bcl11a_1000G = pd.read_csv(f\"results/1000G/BCL11A/{report_fname_samples}\", sep=\"\\t\")\n",
    "bcl11a_HGDP = pd.read_csv(f\"results/HGDP/BCL11A/{report_fname_samples}\", sep=\"\\t\")\n",
    "bcl11a_GNOMAD = pd.read_csv(f\"results/GNOMAD/BCL11A/{report_fname_samples}\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2be77632",
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"1000G\", \"HGDP\", \"GNOMAD\"]\n",
    "data_report_map = {\"1000G\": bcl11a_1000G, \"HGDP\": bcl11a_HGDP, \"GNOMAD\": bcl11a_GNOMAD}\n",
    "\n",
    "f, axes = plt.subplots(1, 3, figsize=(30, 10))\n",
    "\n",
    "for ax, dataset in zip(axes, datasets):\n",
    "    title = f\"BCL11A +58 Erythroid Enhancer - {dataset}\"\n",
    "\n",
    "    # retrieve dataset-specific dataframe\n",
    "    pie_data = retrieve_pie_chart_data(data_report_map[dataset])\n",
    "    piechart(ax, pie_data, title)\n",
    "\n",
    "# single shared legend\n",
    "f.legend(LABELS, loc=\"lower center\", ncol=4, fontsize=16, frameon=False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ef69e1",
   "metadata": {},
   "source": [
    "## Guides On-Target Efficiency and Variant Effect on Alternative On-Targets \n",
    "\n",
    "Here we assess the predicted on-target efficiency of the retrieved guide candidates\n",
    "and investigate how genetic variation influences the presence and scoring of \n",
    "alternative on-targets.\n",
    "\n",
    "For each guide, CRISPR-HAWK computes an on-target efficiency score based on its \n",
    "spacer and PAM sequence. When variants alter either the spacer, the PAM, or both, \n",
    "alternative guide configurations may emerge at the same genomic locus. These \n",
    "alternatives can differ substantially in predicted efficiency compared to the \n",
    "reference guide.\n",
    "\n",
    "The following analyses focus on:\n",
    "\n",
    "- Comparing the efficiency of reference guides against their alternatives\n",
    "\n",
    "- Assessing whether genetoc variants may modulate the expected likelihood of\n",
    "    a guide designed on the reference to work properly on haplotype-specific\n",
    "    on-target sites\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2950ea",
   "metadata": {},
   "source": [
    "Define global constants and special guide identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09614df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "SG1617 = \"chr2_60495261_-\"\n",
    "SAMPLE_COL = \"n_samples\"\n",
    "\n",
    "def compute_guide_id(chrom, start, strand):\n",
    "    return f\"{chrom}_{start}_{strand}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5f4bbf",
   "metadata": {},
   "source": [
    "Calculate score differences between reference and alternative guides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_deltas(df, score_col):\n",
    "    \"\"\"Add delta columns comparing alt to ref scores for each guide.\"\"\"\n",
    "    df = df.copy()\n",
    "    use_abs = score_col != \"score_cfdon\"\n",
    "    \n",
    "    def add_deltas(group):\n",
    "        ref = group[group[\"origin\"] == \"ref\"]\n",
    "        if ref.empty:\n",
    "            return group\n",
    "        \n",
    "        ref_score = ref[score_col].iloc[0]\n",
    "        group[\"delta\"] = group[score_col] - ref_score\n",
    "        if use_abs:\n",
    "            group[\"abs_delta\"] = group[\"delta\"].abs()\n",
    "        return group\n",
    "    \n",
    "    df[\"delta\"] = 0.0\n",
    "    if use_abs:\n",
    "        df[\"abs_delta\"] = 0.0\n",
    "    \n",
    "    return df.groupby(\"guide_id\", group_keys=False).apply(add_deltas)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52028f46",
   "metadata": {},
   "source": [
    "Prepare top-ranked guides based on variant impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30785ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_by_delta(df, score_col, top_n, dataset_type):\n",
    "    \"\"\"Select top N guides with largest variant effects.\"\"\"\n",
    "    df = df.copy()\n",
    "    df[\"guide_id\"] = df.apply(lambda x: compute_guide_id(x[0], x[1], x[6]), axis=1)\n",
    "    df = calculate_deltas(df, score_col)\n",
    "    \n",
    "    # Group guides and extract ref/alt information\n",
    "    guide_data = {}\n",
    "    for guide_id, group in df.groupby(\"guide_id\", sort=False):\n",
    "        ref = group[group[\"origin\"] == \"ref\"]\n",
    "        if ref.empty:\n",
    "            continue\n",
    "        \n",
    "        ref_row = ref.iloc[0]\n",
    "        alts = group[group[\"origin\"] == \"alt\"]\n",
    "        \n",
    "        if score_col == \"score_cfdon\":\n",
    "            alts = alts[alts[score_col] < ref_row[score_col]]\n",
    "        \n",
    "        alt_entries = [\n",
    "            {\n",
    "                \"alt_sgRNA\": alt[\"sgRNA_sequence\"],\n",
    "                \"pam\": alt[\"pam\"],\n",
    "                \"alt_score\": alt[score_col],\n",
    "                \"delta\": alt[\"delta\"],\n",
    "                \"abs_delta\": alt.get(\"abs_delta\", abs(alt[\"delta\"])),\n",
    "                SAMPLE_COL: alt[SAMPLE_COL],\n",
    "                \"variant_id\": alt[\"variant_id\"]\n",
    "            }\n",
    "            for _, alt in alts.iterrows()\n",
    "        ]\n",
    "        \n",
    "        guide_data[guide_id] = {\n",
    "            \"ref_sgRNA\": ref_row[\"sgRNA_sequence\"],\n",
    "            \"pam\": ref_row[\"pam\"],\n",
    "            \"ref_score\": ref_row[score_col],\n",
    "            \"ref_samples\": ref_row[SAMPLE_COL],\n",
    "            \"alts\": alt_entries\n",
    "        }\n",
    "    \n",
    "    # Rank guides by worst variant effect\n",
    "    rankings = []\n",
    "    for gid, data in guide_data.items():\n",
    "        if not data[\"alts\"]:\n",
    "            worst = 0.0\n",
    "        elif score_col == \"score_cfdon\":\n",
    "            worst = min(a[\"delta\"] for a in data[\"alts\"])\n",
    "        else:\n",
    "            worst = max(a[\"abs_delta\"] for a in data[\"alts\"])\n",
    "        rankings.append((gid, worst))\n",
    "    \n",
    "    worst_df = pd.DataFrame(rankings, columns=[\"guide_id\", \"delta\"]) \\\n",
    "        .sort_values(\"delta\", ascending=(score_col == \"score_cfdon\")) \\\n",
    "        .reset_index(drop=True)\n",
    "    \n",
    "    # Include special guide if present\n",
    "    if SG1617 in worst_df[\"guide_id\"].values:\n",
    "        special = worst_df[worst_df[\"guide_id\"] == SG1617]\n",
    "        others = worst_df[worst_df[\"guide_id\"] != SG1617].head(top_n - 1)\n",
    "        final_guides = pd.concat([special, others], ignore_index=True)\n",
    "    else:\n",
    "        final_guides = worst_df.head(top_n)\n",
    "    \n",
    "    final_guides[\"Rank\"] = final_guides.index + 1\n",
    "    \n",
    "    # Build wide-format dataframe\n",
    "    max_alts = max(len(guide_data[gid][\"alts\"]) for gid in final_guides[\"guide_id\"])\n",
    "    rows = []\n",
    "    \n",
    "    for _, row in final_guides.iterrows():\n",
    "        gid = row[\"guide_id\"]\n",
    "        data = guide_data[gid]\n",
    "        \n",
    "        out = {\n",
    "            \"guide_id\": gid,\n",
    "            \"Rank\": row[\"Rank\"],\n",
    "            \"ref_sgRNA\": data[\"ref_sgRNA\"],\n",
    "            \"pam\": data[\"pam\"],\n",
    "            \"ref_score\": data[\"ref_score\"],\n",
    "            \"ref_n_samples\": data[\"ref_samples\"],\n",
    "        }\n",
    "        \n",
    "        for i, alt in enumerate(data[\"alts\"]):\n",
    "            prefix = f\"alt{i+1}_\"\n",
    "            out.update({\n",
    "                f\"{prefix}sgRNA\": alt[\"alt_sgRNA\"],\n",
    "                f\"{prefix}pam\": alt[\"pam\"],\n",
    "                f\"{prefix}score\": alt[\"alt_score\"],\n",
    "                f\"{prefix}delta\": alt[\"delta\"],\n",
    "                f\"{prefix}abs_delta\": alt[\"abs_delta\"],\n",
    "                f\"{prefix}{SAMPLE_COL}\": alt[SAMPLE_COL],\n",
    "                f\"{prefix}variant_id\": alt[\"variant_id\"],\n",
    "            })\n",
    "        \n",
    "        # Fill missing alt columns with NaN\n",
    "        for i in range(len(data[\"alts\"]), max_alts):\n",
    "            prefix = f\"alt{i+1}_\"\n",
    "            for k in [\"sgRNA\", \"pam\", \"score\", \"delta\", \"abs_delta\", SAMPLE_COL, \"variant_id\"]:\n",
    "                out[f\"{prefix}{k}\"] = np.nan\n",
    "        \n",
    "        rows.append(out)\n",
    "    \n",
    "    return pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adc1e37",
   "metadata": {},
   "source": [
    "Utilities for plot sizing and styling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_legend_size(n_samples, dataset_type):\n",
    "    \"\"\"Scale dot size based on sample count and dataset.\"\"\"\n",
    "    if dataset_type == \"GNOMAD\":\n",
    "        thresholds = [1, 20, 50, 150, 500, np.inf]\n",
    "    else:\n",
    "        thresholds = [1, 20, 50, 100, 200, np.inf]\n",
    "    \n",
    "    bases = [1, 10, 35, 75, 150, 300]\n",
    "    for limit, base in zip(thresholds, bases):\n",
    "        if n_samples <= limit:\n",
    "            return 150 * np.sqrt(base)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7879bf",
   "metadata": {},
   "source": [
    "Create a dot plot showing variant effects across guides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dotplot_delta(df, score_col, top_n, dataset_type):\n",
    "    \"\"\"Plot variant effects for top N guides.\"\"\"\n",
    "    top_df = df.sort_values('Rank').head(top_n).copy()\n",
    "    top_df['rank_chr_start'] = top_df.apply(\n",
    "        lambda row: f\"Rank {row['Rank']}, {row['guide_id'].split('_')[0]}:{row['guide_id'].split('_')[1]}\",\n",
    "        axis=1\n",
    "    )\n",
    "    \n",
    "    plt.figure(figsize=(22, 11))\n",
    "    \n",
    "    # Generate color palette for variants\n",
    "    alt_cols = [c for c in df.columns if c.startswith(\"alt\") and c.endswith(f\"_{SAMPLE_COL}\")]\n",
    "    variant_keys = list({\n",
    "        row['rank_chr_start'] for _, row in top_df.iterrows()\n",
    "        for col in alt_cols if pd.notna(row[col]) and row[col] != 'REF'\n",
    "    })\n",
    "    \n",
    "    base_cmaps = ['Purples', 'Blues', 'Greens', 'Oranges', 'Reds',\n",
    "                  'PuRd', 'RdPu', 'BuPu', 'GnBu', 'PuBuGn', 'BuGn',\n",
    "                  'Spectral', 'coolwarm']\n",
    "    palette = [sns.color_palette(cmap, 9)[7] for cmap in base_cmaps]\n",
    "    \n",
    "    if len(variant_keys) > len(palette):\n",
    "        palette += [sns.color_palette(cmap, 9)[i] \n",
    "                    for i in range(6, 9) for cmap in base_cmaps][:len(variant_keys) - len(palette)]\n",
    "    \n",
    "    random.shuffle(palette)\n",
    "    variant_colors = dict(zip(variant_keys, palette))\n",
    "    \n",
    "    # Plot alternative alleles\n",
    "    for _, row in top_df.iterrows():\n",
    "        for i in range(1, 1000):\n",
    "            score_col_alt = f\"alt{i}_score\"\n",
    "            samples_col_alt = f\"alt{i}_{SAMPLE_COL}\"\n",
    "            \n",
    "            if score_col_alt not in row or pd.isna(row[score_col_alt]):\n",
    "                break\n",
    "            \n",
    "            n_samples = int(row[samples_col_alt]) if pd.notna(row[samples_col_alt]) else 1\n",
    "            plt.scatter(\n",
    "                row['Rank'], row[score_col_alt],\n",
    "                color=variant_colors.get(row['rank_chr_start'], 'gray'),\n",
    "                s=get_legend_size(n_samples, dataset_type),\n",
    "                alpha=0.6, edgecolors='white', linewidth=0.5,\n",
    "                marker='D' if n_samples == 1 else 'o', zorder=3\n",
    "            )\n",
    "    \n",
    "    # Plot reference alleles\n",
    "    if score_col == 'score_cfdon':\n",
    "        plt.axhline(y=1, color='gray', linestyle='-', alpha=0.6, linewidth=3, zorder=5)\n",
    "        ref_legend = plt.Line2D([0], [0], color='gray', linewidth=3,\n",
    "                                label='Reference', alpha=0.6)\n",
    "    else:\n",
    "        plt.scatter(top_df['Rank'], top_df['ref_score'],\n",
    "                   color='black', s=600, zorder=5,\n",
    "                   edgecolors='white', linewidth=0.7, label='Reference', alpha=0.6)\n",
    "        ref_legend = plt.Line2D([0], [0], marker='o', color='w', label='Reference',\n",
    "                               markerfacecolor='dimgray', alpha=0.6,\n",
    "                               markersize=np.sqrt(600),\n",
    "                               markeredgecolor='white', linewidth=0.7)\n",
    "    \n",
    "    # Configure axes and labels\n",
    "    ax = plt.gca()\n",
    "    ax.set_xticks(top_df['Rank'])\n",
    "    ax.set_xticklabels([row['ref_sgRNA'] for _, row in top_df.iterrows()],\n",
    "                       rotation=45, ha='right', fontsize=15)\n",
    "    \n",
    "    # Bold special guides\n",
    "    for label, gid in zip(ax.get_xticklabels(), top_df['guide_id'].values):\n",
    "        if gid in [SG1617]:\n",
    "            label.set_weight(\"bold\")\n",
    "    \n",
    "    plt.yticks(fontsize=15)\n",
    "    plt.xlabel('Guide', fontsize=18)\n",
    "    \n",
    "    ylabel = 'Variant Effect (CFD)' if score_col == 'score_cfdon' \\\n",
    "             else f'On-Target Efficiency ({score_col.split(\"_\")[1].upper()})'\n",
    "    title = 'Variant Effect on Alternative On-Targets' if score_col == 'score_cfdon' \\\n",
    "            else 'Guides On-Target Efficiency'\n",
    "    \n",
    "    plt.ylabel(ylabel, fontsize=19)\n",
    "    plt.title(title, fontsize=28)\n",
    "    \n",
    "    # Create legend\n",
    "    legend_labels = ['1', '2–20', '21–50', '51–100', '101–200', '>200']\n",
    "    sample_counts = [1, 10, 35, 75, 150, 300]\n",
    "    markers = ['D'] + ['o'] * 5\n",
    "    scaled_sizes = [150 * np.sqrt(n) for n in sample_counts]\n",
    "    \n",
    "    size_legend_handles = [\n",
    "        plt.Line2D([0], [0], marker=m, color='w', label=l,\n",
    "                   markerfacecolor='gray', markersize=np.sqrt(s), alpha=0.6)\n",
    "        for l, s, m in zip(legend_labels, scaled_sizes, markers)\n",
    "    ]\n",
    "    size_legend_handles.insert(0, ref_legend)\n",
    "    \n",
    "    legend = plt.legend(handles=size_legend_handles, title=\"Dot Size = #Samples\",\n",
    "                       frameon=False, bbox_to_anchor=(0.5, -0.55), loc='lower center',\n",
    "                       ncol=7, fontsize=11, handletextpad=1, columnspacing=5, labelspacing=2)\n",
    "    legend.get_title().set_fontsize(24)\n",
    "    \n",
    "    plt.ylim((-10, 110) if score_col == 'score_deepcpf1' else (-0.05, 1.05))\n",
    "    plt.grid(True, alpha=0.3, linestyle='--')\n",
    "    sns.despine()\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4162c6",
   "metadata": {},
   "source": [
    "Once defined the functions required to plot variants impact on guides efficiency\n",
    "and on-target activity, we focus again on BCL11A +58 Erythroid enhancer. For these\n",
    "analysis we combine the candidate guides using the variants from 1000G, HGDP, and\n",
    "gnomAD datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7948c591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine data from different datasets\n",
    "for df in [bcl11a_1000G, bcl11a_HGDP, bcl11a_GNOMAD]:\n",
    "    if \"n_samples\" not in df.columns:\n",
    "        df[\"n_samples\"] = df[\"n_ref\"] if \"n_ref\" in df.columns else np.nan\n",
    "\n",
    "bcl11a_ALL = pd.concat([\n",
    "    bcl11a_1000G.assign(dataset=\"1000G\"),\n",
    "    bcl11a_HGDP.assign(dataset=\"HGDP\"),\n",
    "    bcl11a_GNOMAD.assign(dataset=\"GNOMAD\")\n",
    "], ignore_index=True)\n",
    "\n",
    "dotplot_delta(\n",
    "    prepare_data_by_delta(bcl11a_ALL, \"score_azimuth\", 25, \"ALL\"),\n",
    "    \"score_azimuth\",\n",
    "    25,\n",
    "    \"ALL\",\n",
    ")\n",
    "dotplot_delta(\n",
    "    prepare_data_by_delta(bcl11a_ALL, \"score_cfdon\", 25, \"ALL\"),\n",
    "    \"score_cfdon\",\n",
    "    25,\n",
    "    \"ALL\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4432008b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-analysis-ngs",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
